---
layout: archive
permalink: /PMRF/
title: "PMRF Review"
author_profile: true
redirect_from: 
  - /pmrf/
  - /pmrf
  - /PMRF
---



My name is Aanjaneya Kumar and I am a PhD student at the [Indian Institute of Science Education and Research Pune](http://www.iiserpune.ac.in/) working under the supervision of [Prof. M. S Santhanam](http://www.iiserpune.ac.in/~santh/). My primary interest is in obtaining insight into practically relevant problems, which could arise in various fields including (but not restricted to) physics, biology, and the social sciences, by first formulating them as mathematical models and then analyzing them using tools from Statistical Physics, Graph Theory and Game Theory. Here is a link to my [CV](https://drive.google.com/file/d/1Z9Pywau_D2Sl3XuqWzL0JYiUxUiikeKp/view?usp=sharing).

A unifying theme of my interests comes from the guiding philosophy behind the several recent developments in the field of Nonequilibrium Statistical Physics. Statistical Physics is a branch of science that deals with systems comprising of a large number of units where we attempt to better understand the phenomena that emerge out of their interactions - many of which are not accessible through reductionist treatments. Systems that are in equilibrium obey the laws of Equilibrium Statistical Physics âˆ’ a subject whose tools are mature and well developed. However, unlike its equilibrium counterpart, Nonequilibrium Statistical Physics does not have an overarching formalism. But in the last three decades, this field has seen a substantial amount of progress. A significant portion of this recent progress has come through the adoption of a kinetic approach. The idea of this approach is to study simple stochastic models specified by dynamical rules which capture the essence of the real process that we are trying to describe. Once the model and its dynamical rules are established, the next step is to critically analyze these simple models, and through this analysis, uncover the new features that the model could possibly tell us about the real process at hand.
The general philosophy described above, in fact, goes much beyond Statistical Physics and can be thought of as a guiding principle for fruitful research across disciplines. I try my best to keep this approach in mind while tackling research problems. 

As a Prime Minister's Research Fellow, I aim to carry out collaborative, interdisciplinary research in the following areas:


# Extreme Events

Extreme events, broadly defined as those events whose numerical values show a pronounced deviation from their typical values and exceed some predefined threshold, are ubiquitous and they occur in a wide variety of natural, social and engineering systems. Not surprisingly, the study of extreme events and systemic failures has received a lot of recent research attention. Starting from market crashes and power blackouts, to extreme climate events and internet breakdown -- extreme events can lead to disastrous consequences in the systems they occur in and severely disrupt their functioning. To this end, a growing body of literature is focused towards building a theoretical framework to analyze such rare events. The classical extreme value statistics is nearly a century old and more recently, advances in large deviation theory have improved our understanding of extreme events. There also has been an interesting line of work, which has aimed to understand how extreme events take place in networked systems. 

In my [first project](https://aip.scitation.org/doi/10.1063/1.5139018) as a PhD student, I explored the problem of extreme events taking place on the *edges* of a network, and showed that they show strikingly different statistics than extreme events taking place on the *nodes* of a network. I computed exactly the statistics of the load and flux on each edge of the network and established that the probability of extreme events occurring on them is given by the regularized incomplete Beta function. Furthermore, I studied time delayed correlations between extreme events occurring on nodes and edges and found that while low degree nodes are more prone to extreme events, carefully observing their neighbouring nodes can lead to identification of precursors. Our results provide a mathematical framework to study network failure through edge deletion mechanism induced by extreme events and provides a systematic route to model the edge deletion rates, based on the underlying dynamical process taking place on the network. Our work was published in the special issue of *Chaos* titled *Rare Events in Complex Systems: Understanding and Prediction*.

Following up on the theme of extreme events, last year we initiated a study of threshold crossing events in the presence of incomplete observations. The problem was motivated by practical applications, where there is an energy cost associated with keeping the "sensor" (which monitors the process of interest) on, and the threshold crossing events in the process of interest are thus monitored by intermittent sensors that are not active at all times. A principal example is of wireless sensor networks, which are widely deployed to monitor rare events at remote locations and operate under tight energy constraints. These sensors are typically not always active in order to optimise power consumption. Other examples where such a scenario arises is where there is simply a lack of data, and the process of interest has only been observed in certain time intervals. Mathematically, our problem can be stated as follows -- consider a time-series *X(t)*, which could be an observable of interest e.g. price of a commodity, temperature of a city, or the damage accumulated in a system. The time taken for *X(t)* to cross a pre-defined threshold is a practically relevant observable called the "first passage time", and it continues to attract active research attention. However, what happens when we cannot always observe *X(t)*, and can only observe it intermittently? In that case, the relevant quantity of interest is the "first detection time", which denotes the first time when the process *X(t)* is *detected* or *observed* to be above the threshold. 

Some preliminary results obtained in the project were presented in the previous PMRF review (held in December 2020). In this project, we studied the general problem of threshold crossing under intermittent sensing using the versatile birth-death process, and a sensor that stochastically switches between active and inactive states, modelled as a two-state Markov process. We obtained a general relation between the first detection time distribution, under intermittent sensing, and the first passage time distribution. One of our central results dictates that the first detection time can be obtained from the first passage times, which is exactly known for a wide range of problems. We demonstrated the validity of our results in a variety of applications including the SIS model of epidemics, logistic model, and also extended our formalism to include birth-death processes which undergo burst like relaxtions and reset to the origin. Intermittent sensing of the process *X(t)* leads to interesting new features -- when the threshold crossing is first detected, the value of *X(t)* need not be exactly equal to the threshold, but can be anywhere in the state space above it. The information about in which state the threshold crossing event is detected is contained in the splitting probabilities, which our formalism allows us to compute exactly. Finally, we considered the problem of inferring first passage times from the knowledge of first detection. We asked -- if the first detection of a threshold crossing event happens at time *T_d*, can we estimate what will be the first passage  time $T_f$? The answer to this question has practical value as it estimates the first occurrence time for an event that possibly went undetected. In sensors that detect abnormal voltage fluctuations (such large voltage fluctuations can damage the device), *T_f* corresponds to the time until which the device being monitored was fully functional (not damaged), but this fluctuation could go unnoticed, and $T_d$ denotes the time when sensor detects the large fluctuation for the first time.  For this problem, we were able to compute the exact first passage distribution, conditioned on the first detection time, and solved the inference problem. 


# Evolutionary Game Theory

Apart from helping us understand biodiveristy, the theory of evolution has given us insight into the evolution of cooperation and languages, as well as the dynamics of virus infections and human cancer. The simple underlying idea behind the theory is that different features among individuals (physical appearance, behaviours, strategies) lead to different (appropriately defined) fitnesses, which eventually leads to a higher relative representation of individuals, whose traits were deemed evolutionarily fit in the past, and hence those traits were successfully passed on to later generations. The question of how cooperation has evolved is an active topic of research, which has benefited a lot from interdisciplinary research in Evolutionary Game Theory. However, a [recent work](https://ora.ox.ac.uk/objects/uuid:8dd8d82d-3829-4857-bcf4-eebf196d11be) shows that cooperation is only one of seven moral behaviours which seem to be universal among different human societies. Inspired by this work, in collaboration with Prof. Valerio Capraro, an Economist from London, and Prof. Matjaz Perc, an interdiscipinary Physicist, I studied the [evolution of trust and trustworthiness](https://royalsocietypublishing.org/doi/10.1098/rsif.2020.0491) using the seminal Trust game, proposed by Berg, Dickhaut and McCabe in 1995, and the replicator dynamics. Our main finding was that trusting other players is not an evolutionarily favoured strategy by itself and thus, there must be other mechanisms in play that lead to the evolution of trust. We are also trying to better understand the evolution of (dis)honesty using tools from network theory and evolutionary game theory. This is a work in progress, the latest draft of this work is available upon request.

A bunch of (related) questions that I would be very interested in viewing from the lens of evolutionary game theory are: Even with the advent of technology, while the facts maybe just a careful Google search away, why does fake news persist for long in our society? Why do retracted papers continue to be cited in scientific literature? Is there a way in which we could combat misinformation?

# Other works
I have also explored some conventional statistical physics problems. The following is a list of such works:
1. [Chase-Escape Percolation on the 2D Square Lattice](https://arxiv.org/abs/2010.05310)
  * With Prof. Deepak Dhar
2. [Improved Upper Bounds on the Asymptotic Growth Velocity of Eden Clusters](http://aanjaneyakumar.com/publication/2020eden)
  * With Prof. Deepak Dhar, in *Journal of Statistical Physics* (2020)
3. [TASEP Speed Process: An Effective Medium Approach](http://aanjaneyakumar.com/publication/2019tasep)
  * With Prof. Deepak Dhar, in *Journal of Statistical Mechanics* (2020)
4. [Distinct Nodes Visited by Random Walkers on Scale-Free Networks](http://aanjaneyakumar.com/publication/2019dsv)
  * With Prof. M. S. Santhanam, in *Physica A* (2019)









